{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check occurrence of test words in corpus\n",
    "\n",
    "This notebook will (1) generate the traing corpora and (2) count the number of occurrences of the words used for the tests in the corpus. The tests are:\n",
    "\n",
    "- training monitor tests: here we have words with upper and lower cases, and possible inflection of words (e.g., in toefl and syn-sem tests we have also plural words, past tenses..)\n",
    "\n",
    "- WEAT test: here all the words corresponds to their lemmas (all lower cased)\n",
    "\n",
    "This means that the two sets of words are different. To count the occurrences of the words of each set, I will build each time two corpora: one keeping the words in their original inflection (used for training monitor tests) and the other lemmatizing all the words (for WEAT test). Counts are computed for each decade and finally aggregated to assess both occurrences per decade and in the whole corpus.\n",
    "\n",
    "We create three corpora:\n",
    "- all person artist song lyrics\n",
    "- all person male artist song lyrics\n",
    "- all person female artist song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version:  3.2.0\n",
      "Spacy pipeline path (for version):  /Users/lorenzo/miniconda3/envs/env_lyrics/lib/python3.9/site-packages/en_core_web_sm/en_core_web_sm-3.2.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "print(\"Spacy version: \", spacy.__version__)\n",
    "print(\"Spacy pipeline path (for version): \", nlp.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_ws(file):\n",
    "    '''\n",
    "    This returns all the unique words of the ws test files.\n",
    "    '''\n",
    "    \n",
    "    words_all = set()\n",
    "    with open(file, 'rt') as rr:\n",
    "        for line in rr:\n",
    "            words = line.strip().split(\"\\t\")[:-1]\n",
    "            words_all.update(words)\n",
    "            \n",
    "    return words_all\n",
    "\n",
    "def read_words_tfl(file):\n",
    "    '''\n",
    "    This returns all the unique words of the tfl test file.\n",
    "    '''\n",
    "    \n",
    "    tfl_test = open(file).read()\n",
    "    words_all = set([w for w in re.split(\"\\n|\\t\", tfl_test) \n",
    "                     if w!='' and '.' not in w and not w[0].isdigit() and len(w)>1])\n",
    "    \n",
    "    return words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dict with test words\n",
    "all_test_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws tests (three tests)\n",
    "# these words are lemmas (no inflections in this test), contains upper cases\n",
    "# example of test (scale of relatedness):\n",
    "#Â computer\tinternet\t7.58\n",
    "\n",
    "# all these tests contains the same set of words! -> No need to store all of them\n",
    "ws353_words = read_words_ws(\"../data/evaluation_tests_word_embedding/ws/ws353.txt\")\n",
    "all_test_words['ws353'] = ws353_words\n",
    "\n",
    "ws353_relatedness_words = read_words_ws(\"../data/evaluation_tests_word_embedding/ws/ws353_relatedness.txt\")\n",
    "#all_test_words['ws353-relatedness'] = ws353_words\n",
    "\n",
    "ws353_similarity_words = read_words_ws(\"../data/evaluation_tests_word_embedding/ws/ws353_similarity.txt\")\n",
    "#all_test_words['ws353-similarity'] = ws353_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toefl test \n",
    "# contains inflected words, no upper cases\n",
    "# example of test (multiple choice):\n",
    "#1.\tenormously\n",
    "#a.\tappropriately\n",
    "#b.\tuniquely\n",
    "#c.\ttremendously\n",
    "#d.\tdecidedly\n",
    "#c\n",
    "\n",
    "tfl_words = read_words_tfl(\"../data/evaluation_tests_word_embedding/tfl/toefl.txt\")\n",
    "all_test_words['toefl'] = tfl_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests (with example):\n",
      "-  capital-common-countries\n",
      "\t Athens Greece Baghdad Iraq\n",
      "-  capital-world\n",
      "\t Abuja Nigeria Accra Ghana\n",
      "-  currency\n",
      "\t Algeria dinar Angola kwanza\n",
      "-  city-in-state\n",
      "\t Chicago Illinois Houston Texas\n",
      "-  family\n",
      "\t boy girl brother sister\n",
      "-  gram1-adjective-to-adverb\n",
      "\t amazing amazingly apparent apparently\n",
      "-  gram2-opposite\n",
      "\t acceptable unacceptable aware unaware\n",
      "-  gram3-comparative\n",
      "\t bad worse big bigger\n",
      "-  gram4-superlative\n",
      "\t bad worst big biggest\n",
      "-  gram5-present-participle\n",
      "\t code coding dance dancing\n",
      "-  gram6-nationality-adjective\n",
      "\t Albania Albanian Argentina Argentinean\n",
      "-  gram7-past-tense\n",
      "\t dancing danced decreasing decreased\n",
      "-  gram8-plural\n",
      "\t banana bananas bird birds\n",
      "-  gram9-plural-verbs\n",
      "\t decrease decreases describe describes\n"
     ]
    }
   ],
   "source": [
    "# different tests based on syntactic and semantic properties\n",
    "# contains inflected words\n",
    "syn_sem_tests = open(\"../data/evaluation_tests_word_embedding/syn_sem/questions-words.txt\", 'rt').read()\n",
    "syn_sem_tests = re.split(\": (\\w+(?:-\\w+)*)\\n\", syn_sem_tests)[1:]\n",
    "syn_sem_tests = {test_name:words.strip() for test_name, words in zip(syn_sem_tests[::2], syn_sem_tests[1::2])}\n",
    "\n",
    "print('All tests (with example):')\n",
    "for test_name, words in syn_sem_tests.items():\n",
    "    print('- ', test_name)\n",
    "    print('\\t', words.split(\"\\n\")[0])\n",
    "    \n",
    "    all_test_words[f'syn_sem-{test_name}'] = set([w.strip() \n",
    "                                                  for line in words.strip().split(\"\\n\") \n",
    "                                                  for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests (with example):\n",
      "- WEAT test name:  EuropeanAmerican_AfricanAmerican_Pleasant_Unpleasant\n",
      "\tAttributes (A and B):\n",
      "\t\tPLEASANT: caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation\n",
      "\n",
      "\t\tUNPLEASANT: abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, bomb, divorce, jail, poverty, ugly, cancer, evil, kill, rotten, vomit\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tEUROPEAN AMERICAN NAMES: adam, chip, harry, josh, roger, alan, frank, ian, justin, ryan, andrew, fred, jack, matthew, stephen, brad, greg, jed, paul, todd, brandon, hank, jonathan, peter, wilbur, amanda, courtney, heather, melanie, sara, amber, crystal, katie, meredith, shannon, betsy, donna, kristin, nancy, stephanie, bobbie-sue, ellen, lauren, peggy, sue-ellen, colleen, emily, megan, rachel, wendy\n",
      "\n",
      "\t\tAFRICAN AMERICAN NAMES: alonzo, jamel, lerone, percell, theo, alphonse, jerome, leroy, rasaan, torrance, darnell, lamar, lionel, rashaun, tvree, deion, lamont, malik, terrence, tyrone, everol, lavon, marcellus, terryl, wardell, aiesha, lashelle, nichelle, shereen, temeka, ebony, latisha, shaniqua, tameisha, teretha, jasmine, latonya, shanise, tanisha, tia, lakisha, latoya, sharise, tashika, yolanda, lashandra, malika, shavonn, tawanda, yvette\n",
      "\n",
      "- WEAT test name:  EuropeanAmerican_AfricanAmerican_Pleasant_Unpleasant_2\n",
      "\tAttributes (A and B):\n",
      "\t\tPLEASANT: caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation, joy, love, peace, wonderful, pleasure, friend, laughter, happy\n",
      "\n",
      "\t\tUNPLEASANT: abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, bomb, divorce, jail, poverty, ugly, cancer, evil, kill, rotten, vomit, agony, terrible, horrible, nasty, evil, war, awful, failure\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tEUROPEAN AMERICAN NAMES: brad, brendan, geoffrey, greg, brett, jay, matthew, neil, todd, allison, anne, carrie, emily, jill, laurie, kristen, meredith, sarah\n",
      "\n",
      "\t\tAFRICAN AMERICAN NAMES: darnell, hakim, jermaine, kareem, jamal, leroy, rasheed, tremayne, tyrone, aisha, ebony, keisha, kenya, latonya, lakisha, latoya, tamika, tanisha\n",
      "\n",
      "- WEAT test name:  Flowers_Insects_Pleasant_Unpleasant\n",
      "\tAttributes (A and B):\n",
      "\t\tPLEASANT: caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation\n",
      "\n",
      "\t\tUNPLEASANT: abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten, vomit, agony, prison\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tFLOWERS: aster, clover, hyacinth, marigold, poppy, azalea, crocus, iris, orchid, rose, bluebell, daffodil, lilac, pansy, tulip, buttercup, daisy, lily, peony, violet, carnation, gladiola, magnolia, petunia, zinnia\n",
      "\n",
      "\t\tINSECTS: ant, caterpillar, flea, locust, spider, bedbug, centipede, fly, maggot, tarantula, bee, cockroach, gnat, mosquito, termite, beetle, cricket, hornet, moth, wasp, blackfly, dragonfly, horsefly, roach, weevil\n",
      "\n",
      "- WEAT test name:  Male_Female_Career_Family\n",
      "\tAttributes (A and B):\n",
      "\t\tCAREER WORDS: executive, management, professional, corporation, salary, office, business, career\n",
      "\n",
      "\t\tFAMILY WORDS: home, parents, children, family, cousins, marriage, wedding, relatives\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMALE NAMES: john, paul, mike, kevin, steve, greg, jeff, bill\n",
      "\n",
      "\t\tFEMALE NAMES: amy, joan, lisa, sarah, diana, kate, ann, donna\n",
      "\n",
      "- WEAT test name:  Math_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: male, man, boy, brother, he, him, his, son\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: female, woman, girl, sister, she, her, hers, daughter\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMATH WORDS: math, algebra, geometry, calculus, equations, computation, numbers, addition\n",
      "\n",
      "\t\tARTS WORDS: poetry, art, dance, literature, novel, symphony, drama, sculpture\n",
      "\n",
      "- WEAT test name:  MusicalInstruments_Weapons_Pleasant_Unpleasant\n",
      "\tAttributes (A and B):\n",
      "\t\tPLEASANT: caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation\n",
      "\n",
      "\t\tUNPLEASANT: abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten, vomit, agony, prison\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMUSICAL INSTRUMENTS: bagpipe, cello, guitar, lute, trombone, banjo, clarinet, harmonica, mandolin, trumpet, bassoon, drum, harp, oboe, tuba, bell, fiddle, harpsichord, piano, viola, bongo, flute, horn, saxophone, violin\n",
      "\n",
      "\t\tWEAPONS: arrow, club, gun, missile, spear, axe, dagger, harpoon, pistol, sword, blade, dynamite, hatchet, rifle, tank, bomb, firearm, knife, shotgun, teargas, cannon, grenade, mace, slingshot, whip\n",
      "\n",
      "- WEAT test name:  Science_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, father, uncle, grandfather, son, he, his, him\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: sister, mother, aunt, grandmother, daughter, she, hers, her\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tSCIENCE WORDS: science, technology, physics, chemistry, einstein, nasa, experiment, astronomy\n",
      "\n",
      "\t\tARTS WORDS: poetry, art, shakespeare, dance, literature, novel, symphony, drama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WEAT test words\n",
    "# contains lemmas\n",
    "weat_weat_file = \"../data/Data_WEAT/weat_attrib_target.json\"\n",
    "weat_associations = json.load(open(weat_weat_file))\n",
    "all_weat_tests = [k for k, v in weat_associations.items() \n",
    "                  if type(v) is dict and 'method' in v.keys() and v['method']=='weat']\n",
    "#all_wefat_tests = [k for k, v in weat_associations.items() if type(v) is dict and 'method' in v.keys() \n",
    "#              and v['method']=='wefat']\n",
    "\n",
    "print('All tests (with example):')\n",
    "for which_test in all_weat_tests:\n",
    "    print('- WEAT test name: ', which_test)\n",
    "    \n",
    "    A_key = weat_associations[which_test]['A_key']\n",
    "    A_words = weat_associations[which_test][A_key]\n",
    "\n",
    "    B_key = weat_associations[which_test]['B_key']\n",
    "    B_words = weat_associations[which_test][B_key]\n",
    "\n",
    "    X_key = weat_associations[which_test]['X_key']\n",
    "    X_words = weat_associations[which_test][X_key]\n",
    "\n",
    "    Y_key = weat_associations[which_test]['Y_key']\n",
    "    Y_words = weat_associations[which_test][Y_key]\n",
    "    \n",
    "    print(\"\\tAttributes (A and B):\")\n",
    "    print(f\"\\t\\t{A_key.upper()}: {', '.join(A_words)}\")\n",
    "    print()\n",
    "    print(f\"\\t\\t{B_key.upper()}: {', '.join(B_words)}\")\n",
    "    print()\n",
    "    print(\"\\tTargets (X and Y):\")\n",
    "    print(f\"\\t\\t{X_key.upper()}: {', '.join(X_words)}\")\n",
    "    print()\n",
    "    print(f\"\\t\\t{Y_key.upper()}: {', '.join(Y_words)}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    all_words_ = set(A_words + B_words + X_words + Y_words)\n",
    "    all_test_words[f'WEAT-{which_test}'] = all_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests (with example):\n",
      "- WEAT test name:  Career_Family_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: male, man, boy, brother, he, him, his, son, father, uncle, grandfather\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tCAREER WORDS: executive, management, professional, corporation, salary, office, business, career\n",
      "\n",
      "\t\tFAMILY WORDS: home, parents, children, family, cousins, marriage, wedding, relatives\n",
      "\n",
      "- WEAT test name:  Math_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: male, man, boy, brother, he, him, his, son, father, uncle, grandfather\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMATH WORDS: math, algebra, geometry, calculus, equations, computation, numbers, addition\n",
      "\n",
      "\t\tARTS WORDS: poetry, art, shakespeare, dance, literature, novel, symphony, drama\n",
      "\n",
      "- WEAT test name:  Science_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: male, man, boy, brother, he, him, his, son, father, uncle, grandfather\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tSCIENCE WORDS: science, technology, physics, chemistry, einstein, nasa, experiment, astronomy\n",
      "\n",
      "\t\tARTS WORDS: poetry, art, shakespeare, dance, literature, novel, symphony, drama\n",
      "\n",
      "- WEAT test name:  Intelligence_Appearance_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: male, man, boy, brother, he, him, his, son, father, uncle, grandfather\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tINTELLIGENCE WORDS: precocious, resourceful, inquisitive, genius, inventive, astute, adaptable, reflective, discerning, intuitive, inquiring, judicious, analytical, apt, venerable, imaginative, shrewd, thoughtful, wise, smart, ingenious, clever, brilliant, logical, intelligent\n",
      "\n",
      "\t\tAPPEARANCE WORDS: alluring, voluptuous, blushing, homely, plump, sensual, gorgeous, slim, bald, athletic, fashionable, stout, ugly, muscular, slender, feeble, handsome, healthy, attractive, fat, weak, thin, pretty, beautiful, strong\n",
      "\n",
      "- WEAT test name:  Strenght_Weakness_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: male, man, boy, brother, he, him, his, son, father, uncle, grandfather\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tSTRENGHT WORDS: power, strong, confident, dominant, potent, command, assert, loud, bold, succeed, triumph, leader, shout, dynamic, winner\n",
      "\n",
      "\t\tWEAKNESS WORDS: weak, surrender, timid, vulnerable, weakness, wispy, withdraw, yield, failure, shy, follow, lose, fragile, afraid, loser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WEAT2 test words\n",
    "# contains lemmas\n",
    "weat2_weat_file = \"../data/Data_WEAT/weat_attrib_target_2.json\"\n",
    "weat2_associations = json.load(open(weat2_weat_file))\n",
    "all_weat2_tests = [k for k, v in weat2_associations.items() \n",
    "                  if type(v) is dict and 'method' in v.keys() and v['method']=='weat']\n",
    "#all_wefat_tests = [k for k, v in weat_associations.items() if type(v) is dict and 'method' in v.keys() \n",
    "#              and v['method']=='wefat']\n",
    "\n",
    "print('All tests (with example):')\n",
    "for which_test in all_weat2_tests:\n",
    "    print('- WEAT test name: ', which_test)\n",
    "    \n",
    "    A_key = weat2_associations[which_test]['A_key']\n",
    "    A_words = weat2_associations[which_test][A_key]\n",
    "\n",
    "    B_key = weat2_associations[which_test]['B_key']\n",
    "    B_words = weat2_associations[which_test][B_key]\n",
    "\n",
    "    X_key = weat2_associations[which_test]['X_key']\n",
    "    X_words = weat2_associations[which_test][X_key]\n",
    "\n",
    "    Y_key = weat2_associations[which_test]['Y_key']\n",
    "    Y_words = weat2_associations[which_test][Y_key]\n",
    "    \n",
    "    print(\"\\tAttributes (A and B):\")\n",
    "    print(f\"\\t\\t{A_key.upper()}: {', '.join(A_words)}\")\n",
    "    print()\n",
    "    print(f\"\\t\\t{B_key.upper()}: {', '.join(B_words)}\")\n",
    "    print()\n",
    "    print(\"\\tTargets (X and Y):\")\n",
    "    print(f\"\\t\\t{X_key.upper()}: {', '.join(X_words)}\")\n",
    "    print()\n",
    "    print(f\"\\t\\t{Y_key.upper()}: {', '.join(Y_words)}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    all_words_ = set(A_words + B_words + X_words + Y_words)\n",
    "    all_test_words[f'WEAT2-{which_test}'] = all_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests (with example):\n",
      "- WEAT test name:  Flowers_Insects_Pleasant_Unpleasant\n",
      "\tAttributes (A and B):\n",
      "\t\tPLEASANT: family, honest, gift, wonderful, vacation, miracle, loyal, pleasure, gentle, rainbow, love, peace, lucky, honor, freedom, happy, health, friend, laughter, cheer, joy, heaven, diploma, paradise, diamond, caress, sunrise\n",
      "\n",
      "\t\tUNPLEASANT: tragedy, awful, pollute, failure, agony, poison, terrible, poverty, grief, disaster, nasty, filth, murder, horrible, ugly, evil, rotten, jail, vomit, assault, sickness, accident, crash, divorce, cancer, hatred, kill, abuse, bomb, death, prison, war, stink\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tFLOWERS: lilac, bluebell, violet, zinnia, peony, crocus, buttercup, iris, rose, pansy, tulip, aster, daisy, azalea, marigold, hyacinth, daffodil, orchid, petunia, carnation, magnolia, lily, gladiola, poppy, clover\n",
      "\n",
      "\t\tINSECTS: cockroach, wasp, bedbug, ant, beetle, gnat, flea, centipede, tarantula, blackfly, fly, locust, horsefly, caterpillar, spider, roach, mosquito, dragonfly, weevil, bee, hornet, maggot, moth, cricket, termite\n",
      "\n",
      "- WEAT test name:  Male_Female_Career_Family\n",
      "\tAttributes (A and B):\n",
      "\t\tCAREER WORDS: corporation, professional, executive, business, office, management, salary, career\n",
      "\n",
      "\t\tFAMILY WORDS: relatives, family, marriage, wedding, parents, cousins, children, home\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMALE NAMES: paul, kevin, steve, greg, jeff, john, bill, mike\n",
      "\n",
      "\t\tFEMALE NAMES: diana, kate, joan, sarah, donna, lisa, amy, ann\n",
      "\n",
      "- WEAT test name:  Math_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, grandfather, his, son, father, man, male, uncle, he, him, boy\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: girl, hers, her, aunt, daughter, sister, female, mother, she, grandmother, woman\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMATH WORDS: addition, calculus, computation, algebra, math, numbers, geometry, equations\n",
      "\n",
      "\t\tARTS WORDS: poetry, novel, symphony, art, dance, shakespeare, sculpture, drama, literature\n",
      "\n",
      "- WEAT test name:  MusicalInstruments_Weapons_Pleasant_Unpleasant\n",
      "\tAttributes (A and B):\n",
      "\t\tPLEASANT: family, honest, gift, wonderful, vacation, miracle, loyal, pleasure, gentle, rainbow, love, peace, lucky, honor, freedom, happy, health, friend, laughter, cheer, joy, heaven, diploma, paradise, diamond, caress, sunrise\n",
      "\n",
      "\t\tUNPLEASANT: tragedy, awful, pollute, failure, agony, poison, terrible, poverty, grief, disaster, nasty, filth, murder, horrible, ugly, evil, rotten, jail, vomit, assault, sickness, accident, crash, divorce, cancer, hatred, kill, abuse, bomb, death, prison, war, stink\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMUSICAL INSTRUMENTS: banjo, mandolin, trombone, cello, fiddle, bagpipe, tuba, bassoon, harmonica, harp, violin, piano, trumpet, clarinet, oboe, guitar, lute, saxophone, horn, bongo, flute, harpsichord, bell, viola, drum\n",
      "\n",
      "\t\tWEAPONS: harpoon, arrow, missile, firearm, whip, hatchet, gun, dagger, pistol, slingshot, teargas, knife, club, rifle, bomb, axe, cannon, mace, shotgun, tank, sword, blade, grenade, dynamite, spear\n",
      "\n",
      "- WEAT test name:  Science_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, grandfather, his, son, father, man, male, uncle, he, him, boy\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: girl, hers, her, aunt, daughter, sister, female, mother, she, grandmother, woman\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tSCIENCE WORDS: astronomy, nasa, einstein, chemistry, experiment, physics, technology, science\n",
      "\n",
      "\t\tARTS WORDS: poetry, novel, symphony, art, dance, shakespeare, sculpture, drama, literature\n",
      "\n",
      "- WEAT test name:  Career_Family_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, grandfather, his, son, father, man, male, uncle, he, him, boy\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: girl, hers, her, aunt, daughter, sister, female, mother, she, grandmother, woman\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tCAREER WORDS: corporation, professional, executive, business, office, management, salary, career\n",
      "\n",
      "\t\tFAMILY WORDS: relatives, family, marriage, wedding, parents, cousins, children, home\n",
      "\n",
      "- WEAT test name:  Intelligence_Appearance_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, grandfather, his, son, father, man, male, uncle, he, him, boy\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: girl, hers, her, aunt, daughter, sister, female, mother, she, grandmother, woman\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tINTELLIGENCE WORDS: brilliant, inquisitive, logical, inventive, judicious, imaginative, apt, astute, smart, venerable, thoughtful, wise, ingenious, precocious, genius, intelligent, shrewd, reflective, resourceful, adaptable, discerning, analytical, clever, inquiring, intuitive\n",
      "\n",
      "\t\tAPPEARANCE WORDS: slim, thin, attractive, feeble, healthy, strong, voluptuous, bald, sensual, fashionable, athletic, stout, handsome, ugly, fat, beautiful, pretty, slender, alluring, blushing, plump, homely, muscular, weak, gorgeous\n",
      "\n",
      "- WEAT test name:  Strenght_Weakness_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, grandfather, his, son, father, man, male, uncle, he, him, boy\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: girl, hers, her, aunt, daughter, sister, female, mother, she, grandmother, woman\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tSTRENGHT WORDS: assert, triumph, confident, potent, loud, winner, shout, succeed, strong, bold, leader, dominant, dynamic, command, power\n",
      "\n",
      "\t\tWEAKNESS WORDS: vulnerable, yield, withdraw, loser, fragile, afraid, wispy, lose, timid, follow, failure, surrender, weak, weakness, shy\n",
      "\n",
      "- WEAT test name:  Matsci_Arts_Male_Female\n",
      "\tAttributes (A and B):\n",
      "\t\tMALE ATTRIBUTES: brother, grandfather, his, son, father, man, male, uncle, he, him, boy\n",
      "\n",
      "\t\tFEMALE ATTRIBUTES: girl, hers, her, aunt, daughter, sister, female, mother, she, grandmother, woman\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMATSCI WORDS: astronomy, nasa, einstein, chemistry, experiment, physics, technology, science, addition, calculus, computation, algebra, math, numbers, geometry, equations\n",
      "\n",
      "\t\tARTS WORDS: poetry, novel, symphony, art, dance, shakespeare, sculpture, drama, literature\n",
      "\n",
      "- WEAT test name:  Malenew_Femalenew_Career_Family\n",
      "\tAttributes (A and B):\n",
      "\t\tCAREER WORDS: corporation, professional, executive, business, office, management, salary, career\n",
      "\n",
      "\t\tFAMILY WORDS: relatives, family, marriage, wedding, parents, cousins, children, home\n",
      "\n",
      "\tTargets (X and Y):\n",
      "\t\tMALE NAMES NEW: john, jack, joe, johnny, james, david, paul, billy, jimmy, simon, mark, romeo, bill, peter, bob, lee, jim, bobby, tom, jackson, sam, michael, charlie, adam\n",
      "\n",
      "\t\tFEMALE NAMES NEW: kim, rose, mary, eve, kelly, jane, lisa, juliet, jean, annie, trina, sarah, sally, betty, lucy, taylor, bonnie, marie, jenny, dolly, julia, anna, jill, angie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WEAT3 test words\n",
    "# contains lemmas\n",
    "weat3_weat_file = \"../data/Data_WEAT/weat_attrib_target_3.json\"\n",
    "weat3_associations = json.load(open(weat3_weat_file))\n",
    "all_weat3_tests = [k for k, v in weat3_associations.items() \n",
    "                  if type(v) is dict and 'method' in v.keys() and v['method']=='weat']\n",
    "#all_wefat_tests = [k for k, v in weat_associations.items() if type(v) is dict and 'method' in v.keys() \n",
    "#              and v['method']=='wefat']\n",
    "\n",
    "print('All tests (with example):')\n",
    "for which_test in all_weat3_tests:\n",
    "    print('- WEAT test name: ', which_test)\n",
    "    \n",
    "    A_key = weat3_associations[which_test]['A_key']\n",
    "    A_words = weat3_associations[which_test][A_key]\n",
    "\n",
    "    B_key = weat3_associations[which_test]['B_key']\n",
    "    B_words = weat3_associations[which_test][B_key]\n",
    "\n",
    "    X_key = weat3_associations[which_test]['X_key']\n",
    "    X_words = weat3_associations[which_test][X_key]\n",
    "\n",
    "    Y_key = weat3_associations[which_test]['Y_key']\n",
    "    Y_words = weat3_associations[which_test][Y_key]\n",
    "    \n",
    "    print(\"\\tAttributes (A and B):\")\n",
    "    print(f\"\\t\\t{A_key.upper()}: {', '.join(A_words)}\")\n",
    "    print()\n",
    "    print(f\"\\t\\t{B_key.upper()}: {', '.join(B_words)}\")\n",
    "    print()\n",
    "    print(\"\\tTargets (X and Y):\")\n",
    "    print(f\"\\t\\t{X_key.upper()}: {', '.join(X_words)}\")\n",
    "    print()\n",
    "    print(f\"\\t\\t{Y_key.upper()}: {', '.join(Y_words)}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    all_words_ = set(A_words + B_words + X_words + Y_words)\n",
    "    all_test_words[f'WEAT3-{which_test}'] = all_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to create linesentence files from lyrics corpus\n",
    "\n",
    "def load_contractions(file):\n",
    "    '''\n",
    "    Load a dictionary of expansion of contractions (ex. he'll: he will / he shall). This function will load this file as a dictionary where contractions are keys and the first expansion is the value.\n",
    "    \n",
    "    ATTENTION TO APOSTROPHE CHARACTER!!\n",
    "    \n",
    "    Input:\n",
    "    file : str, the file where there are the contractions\n",
    "    \n",
    "    Returns:\n",
    "    CONTRACTIONS : dict of strings (contr:expa), couple contraction and expansion\n",
    "    '''\n",
    "    \n",
    "    CONTRACTIONS = []\n",
    "    with open(file, 'rt') as rr:\n",
    "        for line in rr:\n",
    "            line = line.split(':')\n",
    "            CONTRACTIONS.append({line[0].strip().lower() : line[1].split('/')[0].strip()})\n",
    "            CONTRACTIONS.append({line[0].strip().capitalize() : line[1].split('/')[0].strip().capitalize()})\n",
    "\n",
    "    CONTRACTIONS = {cont:exp for dic in CONTRACTIONS for cont,exp in dic.items()}\n",
    "    \n",
    "    return CONTRACTIONS\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_dict):\n",
    "    '''\n",
    "    Given a text, expand all its contracted form.\n",
    "    \n",
    "    Inputs:\n",
    "    text: str, a text\n",
    "    contraction_dict : dict of strings, (contraction:expansion)\n",
    "    \n",
    "    Returns:\n",
    "    text : str, the text with expanded contractions\n",
    "    '''\n",
    "    \n",
    "    #contraction_in_text = re.findall(\"\\w+[']\\w+\", text)\n",
    "        \n",
    "    contraction_in_text = re.findall(r\"\\b(?:{})\\b\".format(\"|\".join(contraction_dict.keys())),\n",
    "                                    text)\n",
    "    \n",
    "    if len(contraction_in_text)==0:\n",
    "        return text\n",
    "    \n",
    "    expansion_pairs = [(contraction, contraction_dict[contraction]) for contraction in contraction_in_text]\n",
    "    \n",
    "    for con, exp in expansion_pairs:\n",
    "        #text = text.replace(con, exp)\n",
    "        text = re.sub(r\"\\b{}\\b\".format(con), exp, text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_consecutive_repeated_lines(lyric):\n",
    "    '''\n",
    "    Remove consecutive repeated lines across the lyric. \n",
    "    \n",
    "    Preserve the stanza structure.\n",
    "    '''\n",
    "    \n",
    "    lines = lyric.strip().split('\\n')\n",
    "    new_lines = [lines[0]]\n",
    "    for line in lines[1:]:\n",
    "        \n",
    "        if line==new_lines[-1]:\n",
    "            # it is a repeated line\n",
    "            continue \n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "        \n",
    "    lyric_new = '\\n'.join(new_lines)\n",
    "    return lyric_new\n",
    "\n",
    "def spacy_lemmatizer_single_text(text, nlp):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    tokens = [get_lemma(token) for token in doc]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def spacy_get_lemma(token):\n",
    "        lemma = token.lemma_\n",
    "        lemma = lemma if lemma!='-PRON-' else token.text\n",
    "        return lemma\n",
    "    \n",
    "    \n",
    "def process_lyrics(text, lower=False, lemmatize=False):\n",
    "    \n",
    "    vocab_count = defaultdict(int)\n",
    "    if lemmatize:\n",
    "        vocab_count_lemma = defaultdict(int)\n",
    "    \n",
    "    text = text.replace(\"`\", \"'\").replace(\"â\", \"'\")\n",
    "    text = expand_contractions(text, contraction_dict)\n",
    "    text = remove_consecutive_repeated_lines(text)\n",
    "    \n",
    "    # remove newline chars and remove additional spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # tokenize\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.is_punct==False and token.is_alpha]\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens_lemma = [spacy_get_lemma(token) for token in doc if token.is_punct==False and token.is_alpha]\n",
    "\n",
    "    if lower:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        if lemmatize:\n",
    "            tokens_lemma = [t.lower() for t in tokens_lemma]\n",
    "            \n",
    "    # remove repeated tokens\n",
    "    tokens = [tokens[0]] + [token_post for token_pre, token_post in zip(tokens[:-1], tokens[1:]) \n",
    "                          if token_pre!=token_post]\n",
    "    if lemmatize:\n",
    "        tokens_lemma = [tokens_lemma[0]] + [token_post for token_pre, token_post in zip(tokens_lemma[:-1], tokens_lemma[1:]) \n",
    "                          if token_pre!=token_post]\n",
    "        \n",
    "    # count occurrences of tokens\n",
    "    for token in tokens:\n",
    "        vocab_count[token] += 1\n",
    "    if lemmatize:\n",
    "        for token in tokens_lemma:\n",
    "            vocab_count_lemma[token] += 1\n",
    "\n",
    "    line = \" \".join(tokens)\n",
    "    if lemmatize:\n",
    "        line_lemma = \" \".join(tokens_lemma)\n",
    "\n",
    "    if lemmatize:\n",
    "        return vocab_count, vocab_count_lemma, line, line_lemma\n",
    "    else:\n",
    "        return vocab_count, line\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = load_contractions(\"../data/contractions_eng.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade 1960...\n",
      "Processing decade 1970...\n",
      "Processing decade 1980...\n",
      "Processing decade 1990...\n",
      "Processing decade 2000...\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create folders to store the corpus corpus and save it\n",
    "if not os.path.exists('../data/corpora'):\n",
    "    os.mkdir('../data/corpora')\n",
    "\n",
    "if not os.path.exists('../data/corpora_lemmatized'):\n",
    "    os.mkdir('../data/corpora_lemmatized')\n",
    "    \n",
    "decades = ['1960', '1970', '1980', '1990', '2000']\n",
    "lower = True\n",
    "\n",
    "vocab_count_all_person = defaultdict(int)\n",
    "vocab_count_male_person = defaultdict(int)\n",
    "vocab_count_female_person = defaultdict(int)\n",
    "\n",
    "vocab_count_all_person_lemma = defaultdict(int)\n",
    "vocab_count_male_person_lemma = defaultdict(int)\n",
    "vocab_count_female_person_lemma = defaultdict(int)\n",
    "\n",
    "# open files\n",
    "out_file_corpus_all_person = open(\"../data/corpora/all_person_artist_lyrics.cor\", 'wt')\n",
    "out_file_corpus_male_person = open(\"../data/corpora/male_person_artist_lyrics.cor\", 'wt')\n",
    "out_file_corpus_female_person = open(\"../data/corpora/female_person_artist_lyrics.cor\", 'wt')\n",
    "\n",
    "out_file_corpus_all_person_lemma = open(\"../data/corpora_lemmatized/all_person_artist_lyrics.cor\", 'wt')\n",
    "out_file_corpus_male_person_lemma = open(\"../data/corpora_lemmatized/male_person_artist_lyrics.cor\", 'wt')\n",
    "out_file_corpus_female_person_lemma = open(\"../data/corpora_lemmatized/female_person_artist_lyrics.cor\", 'wt')\n",
    "\n",
    "out_file_song_ids_all_person = open(\"../data/info_copus_lyrics_all_person.txt\", 'wt')\n",
    "out_file_song_ids_male_person = open(\"../data/info_copus_lyrics_male_person.txt\", 'wt')\n",
    "out_file_song_ids_female_person = open(\"../data/info_copus_lyrics_female_person.txt\", 'wt')\n",
    "\n",
    "for decade in decades:\n",
    "    \n",
    "    print(f'Processing decade {decade}...')\n",
    "    \n",
    "    # load all the lyrics of the decade (only person)\n",
    "    lyrics_artist = pd.read_json(f\"../data/dataset_10_no_duplicates/data_lyrics_person_decades/lyrics_{decade}.json.gz\",\n",
    "                                orient='records', lines=True)\n",
    "    lyrics_artist = lyrics_artist[['artist_id', 'song_id', 'lyrics', 'other_artist_info']]\n",
    "    lyrics_artist.loc[:, 'gender'] = lyrics_artist.other_artist_info.apply(lambda a_info: a_info['gender'])\n",
    "    lyrics_artist.loc[:, 'artist_type'] = 'Person'\n",
    "    \n",
    "    lyrics = lyrics_artist.reset_index(drop=True)\n",
    "    \n",
    "    for idx, row in lyrics.iterrows():\n",
    "        \n",
    "        song_id = row.song_id\n",
    "        artist_gender = row.gender\n",
    "        artist_type = row.artist_type\n",
    "        lyrics = row.lyrics\n",
    "        \n",
    "        # process lyrics\n",
    "        vocab_count, vocab_count_lemma, line, line_lemma = process_lyrics(lyrics, \n",
    "                                                                          lower=lower, \n",
    "                                                                          lemmatize=True)\n",
    "        \n",
    "        if line.strip()=='' or line_lemma.strip()=='':\n",
    "            print(f\"Song ID {song_id} has empty lyrics..\")\n",
    "            continue\n",
    "        \n",
    "        # update vocab counts\n",
    "        for w, count in vocab_count.items():\n",
    "            vocab_count_all_person[w] += count\n",
    "            if artist_gender=='Male':\n",
    "                vocab_count_male_person[w] += count\n",
    "            elif artist_gender=='Female':\n",
    "                vocab_count_female_person[w] += count\n",
    "                \n",
    "        for w, count in vocab_count_lemma.items():\n",
    "            vocab_count_all_person_lemma[w] += count\n",
    "            if artist_gender=='Male':\n",
    "                vocab_count_male_person_lemma[w] += count\n",
    "            elif artist_gender=='Female':\n",
    "                vocab_count_female_person_lemma[w] += count \n",
    "                \n",
    "        # write lines\n",
    "        out_file_corpus_all_person.write(line+'\\n')\n",
    "        out_file_corpus_all_person_lemma.write(line_lemma+'\\n')\n",
    "        out_file_song_ids_all_person.write(song_id+'\\n')\n",
    "        \n",
    "        if artist_gender=='Male':\n",
    "            out_file_corpus_male_person.write(line+'\\n')\n",
    "            out_file_corpus_male_person_lemma.write(line_lemma+'\\n')\n",
    "            out_file_song_ids_male_person.write(song_id+'\\n')\n",
    "        elif artist_gender=='Female':\n",
    "            out_file_corpus_female_person.write(line+'\\n')\n",
    "            out_file_corpus_female_person_lemma.write(line_lemma+'\\n')\n",
    "            out_file_song_ids_female_person.write(song_id+'\\n')\n",
    "            \n",
    "            \n",
    "# close all files\n",
    "out_file_corpus_all_person.close()\n",
    "out_file_corpus_male_person.close()\n",
    "out_file_corpus_female_person.close()\n",
    "\n",
    "out_file_corpus_all_person_lemma.close()\n",
    "out_file_corpus_male_person_lemma.close()\n",
    "out_file_corpus_female_person_lemma.close()\n",
    "\n",
    "out_file_song_ids_all_person.close()\n",
    "out_file_song_ids_male_person.close()\n",
    "out_file_song_ids_female_person.close()\n",
    "    \n",
    "    \n",
    "# now count words for each test\n",
    "word_tests_occurrence = []\n",
    "for test_name, word_set in all_test_words.items():\n",
    "\n",
    "    if '-' in test_name:\n",
    "        test_name, test_type = test_name.split('-')[0], test_name.split('-')[1:]\n",
    "        test_type = '-'.join(test_type)\n",
    "    else:\n",
    "        test_name, test_type = test_name, ''\n",
    "\n",
    "    if lower:\n",
    "        word_set = set([w.lower() for w in word_set]) \n",
    "        \n",
    "        \n",
    "    # get how many times test words occurs in corpora\n",
    "    test_word_count_all_person = {w:vocab_count_all_person[w] for w in word_set}\n",
    "    test_word_count_all_person_lemma = {w:vocab_count_all_person_lemma[w] for w in word_set}\n",
    "    \n",
    "    test_word_count_male_person = {w:vocab_count_male_person[w] for w in word_set}\n",
    "    test_word_count_male_person_lemma = {w:vocab_count_male_person_lemma[w] for w in word_set}\n",
    "    \n",
    "    test_word_count_female_person = {w:vocab_count_female_person[w] for w in word_set}\n",
    "    test_word_count_female_person_lemma = {w:vocab_count_female_person_lemma[w] for w in word_set}\n",
    "    \n",
    "    # append all\n",
    "    word_tests_occurrence.append({\n",
    "        'test_name':test_name,\n",
    "        'test_type':test_type,\n",
    "        'test_word_count_all_person':test_word_count_all_person,\n",
    "        'test_word_count_all_person_lemmatized':test_word_count_all_person_lemma,\n",
    "        'test_word_count_male_person':test_word_count_male_person,\n",
    "        'test_word_count_male_person_lemmatized':test_word_count_male_person_lemma,\n",
    "        'test_word_count_female_person':test_word_count_female_person,\n",
    "        'test_word_count_female_person_lemmatized':test_word_count_female_person_lemma,\n",
    "    })\n",
    "\n",
    "word_tests_occurrence = pd.DataFrame(word_tests_occurrence)\n",
    "word_tests_occurrence.to_json(f'../data/occurrence_test_words_in_person_corpora.json')\n",
    "print('*-'*20+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the word counts\n",
    "json.dump(vocab_count_all_person, open(\"../data/vocab_count_all_person.json\", 'wt'))\n",
    "json.dump(vocab_count_male_person, open(\"../data/vocab_count_male_person.json\", 'wt'))\n",
    "json.dump(vocab_count_female_person, open(\"../data/vocab_count_female_person.json\", 'wt'))\n",
    "\n",
    "json.dump(vocab_count_all_person_lemma, open(\"../data/vocab_count_all_person_lemma.json\", 'wt'))\n",
    "json.dump(vocab_count_male_person_lemma, open(\"../data/vocab_count_male_person_lemma.json\", 'wt'))\n",
    "json.dump(vocab_count_female_person_lemma, open(\"../data/vocab_count_female_person_lemma.json\", 'wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lyrics",
   "language": "python",
   "name": "env_lyrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
