{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10250,
     "status": "ok",
     "timestamp": 1643472724804,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "035rIb3DNM1d",
    "outputId": "2a569d7f-6956-41b8-a4ce-fbb5a35cd212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Unidecode\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█▍                              | 10 kB 27.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 20 kB 33.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30 kB 39.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 40 kB 43.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 51 kB 32.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 61 kB 35.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 71 kB 25.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 81 kB 26.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 92 kB 27.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 102 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 112 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 122 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 133 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 143 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 153 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 163 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 174 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 184 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 194 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 204 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 215 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 225 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 29.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 29.7 MB/s \n",
      "\u001b[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.3.2\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 26.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 61.9 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 6.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 58.9 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 69.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Unidecode\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16336,
     "status": "ok",
     "timestamp": 1643472828108,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "U02R6ElYDJVD",
    "outputId": "9527f346-da1c-44e9-a071-b09541ce0a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7127,
     "status": "ok",
     "timestamp": 1643455676080,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "JuUZqRGIKsoJ"
   },
   "outputs": [],
   "source": [
    "# copy code\n",
    "!cp -r /content/drive/MyDrive/WASABI_gender_experiments/utils/reproduce_sexism_classifier/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1643455677294,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "VD_WT9DxO3ez",
    "outputId": "c778423e-d40d-413a-d4c2-93714191d78f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486711,
     "status": "ok",
     "timestamp": 1643456168615,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "UHL1-I3-NOex",
    "outputId": "669dc3cc-48ed-4a91-fac0-5a91276e329a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "dataset: bho\n",
      "dataset: callme\n",
      "dataset: scales complete\n",
      "dataset: bh\n",
      "dataset: scales train\n",
      "Downloading: 100% 28.0/28.0 [00:00<00:00, 19.2kB/s]\n",
      "Downloading: 100% 226k/226k [00:00<00:00, 1.61MB/s]\n",
      "Downloading: 100% 455k/455k [00:00<00:00, 2.57MB/s]\n",
      "Downloading: 100% 570/570 [00:00<00:00, 530kB/s]\n",
      "Downloading: 100% 420M/420M [00:12<00:00, 35.7MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "training bert_finetuned on adversarial omnibus\n",
      "Beginning Training!\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      " 33% 32/96 [00:17<00:28,  2.24it/s]Saving model checkpoint to checkpoints/iter_0/checkpoint-32\n",
      "Configuration saved in checkpoints/iter_0/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints/iter_0/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_0/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_0/checkpoint-32/special_tokens_map.json\n",
      " 67% 64/96 [00:39<00:14,  2.19it/s]Saving model checkpoint to checkpoints/iter_0/checkpoint-64\n",
      "Configuration saved in checkpoints/iter_0/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints/iter_0/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_0/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_0/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_0/checkpoint-32] due to args.save_total_limit\n",
      "100% 96/96 [01:03<00:00,  2.11it/s]Saving model checkpoint to checkpoints/iter_0/checkpoint-96\n",
      "Configuration saved in checkpoints/iter_0/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints/iter_0/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_0/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_0/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_0/checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 75.0003, 'train_samples_per_second': 40.0, 'train_steps_per_second': 1.28, 'train_loss': 0.5302295287450155, 'epoch': 3.0}\n",
      "100% 96/96 [01:14<00:00,  1.28it/s]\n",
      "Training took time  0:01:15.009949\n",
      "***** Running Prediction *****\n",
      "  Num examples = 602\n",
      "  Batch size = 8\n",
      " 97% 74/76 [00:05<00:00, 14.07it/s]iteration 1\n",
      "dataset: bho\n",
      "dataset: callme\n",
      "dataset: scales complete\n",
      "dataset: bh\n",
      "dataset: scales train\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "training bert_finetuned on adversarial omnibus\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Beginning Training!\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      "\n",
      "  0% 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "  1% 1/96 [00:00<00:50,  1.89it/s]\u001b[A\n",
      "  2% 2/96 [00:01<00:50,  1.86it/s]\u001b[A\n",
      "  3% 3/96 [00:01<00:50,  1.85it/s]\u001b[A\n",
      "  4% 4/96 [00:02<00:50,  1.81it/s]\u001b[A\n",
      "  5% 5/96 [00:02<00:50,  1.80it/s]\u001b[A\n",
      "  6% 6/96 [00:03<00:49,  1.81it/s]\u001b[A\n",
      "  7% 7/96 [00:03<00:49,  1.81it/s]\u001b[A\n",
      "  8% 8/96 [00:04<00:48,  1.82it/s]\u001b[A\n",
      "  9% 9/96 [00:04<00:47,  1.82it/s]\u001b[A\n",
      " 10% 10/96 [00:05<00:47,  1.82it/s]\u001b[A\n",
      " 11% 11/96 [00:06<00:46,  1.82it/s]\u001b[A\n",
      " 12% 12/96 [00:06<00:46,  1.82it/s]\u001b[A\n",
      "100% 76/76 [00:21<00:00, 14.07it/s]\n",
      " 15% 14/96 [00:07<00:45,  1.82it/s]\u001b[A\n",
      " 16% 15/96 [00:08<00:44,  1.83it/s]\u001b[A\n",
      " 17% 16/96 [00:08<00:44,  1.79it/s]\u001b[A\n",
      " 18% 17/96 [00:09<00:43,  1.80it/s]\u001b[A\n",
      " 19% 18/96 [00:09<00:43,  1.81it/s]\u001b[A\n",
      " 20% 19/96 [00:10<00:42,  1.82it/s]\u001b[A\n",
      " 21% 20/96 [00:11<00:41,  1.82it/s]\u001b[A\n",
      " 22% 21/96 [00:11<00:41,  1.83it/s]\u001b[A\n",
      " 23% 22/96 [00:12<00:40,  1.82it/s]\u001b[A\n",
      " 24% 23/96 [00:12<00:40,  1.82it/s]\u001b[A\n",
      " 25% 24/96 [00:13<00:39,  1.81it/s]\u001b[A\n",
      " 26% 25/96 [00:13<00:38,  1.82it/s]\u001b[A\n",
      " 27% 26/96 [00:14<00:38,  1.82it/s]\u001b[A\n",
      " 28% 27/96 [00:14<00:37,  1.82it/s]\u001b[A\n",
      " 29% 28/96 [00:15<00:37,  1.82it/s]\u001b[A\n",
      " 30% 29/96 [00:15<00:36,  1.82it/s]\u001b[A\n",
      " 31% 30/96 [00:16<00:36,  1.83it/s]\u001b[A\n",
      " 32% 31/96 [00:17<00:35,  1.83it/s]\u001b[A\n",
      " 33% 32/96 [00:17<00:29,  2.19it/s]\u001b[ASaving model checkpoint to checkpoints/iter_1/checkpoint-32\n",
      "Configuration saved in checkpoints/iter_1/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints/iter_1/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_1/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_1/checkpoint-32/special_tokens_map.json\n",
      "\n",
      " 34% 33/96 [00:22<01:56,  1.86s/it]\u001b[A\n",
      " 35% 34/96 [00:22<01:30,  1.47s/it]\u001b[A\n",
      " 36% 35/96 [00:23<01:12,  1.19s/it]\u001b[A\n",
      " 38% 36/96 [00:24<01:00,  1.00s/it]\u001b[A\n",
      " 39% 37/96 [00:24<00:51,  1.16it/s]\u001b[A\n",
      " 40% 38/96 [00:25<00:44,  1.30it/s]\u001b[A\n",
      " 41% 39/96 [00:25<00:39,  1.43it/s]\u001b[A\n",
      " 42% 40/96 [00:26<00:36,  1.53it/s]\u001b[A\n",
      " 43% 41/96 [00:26<00:34,  1.61it/s]\u001b[A\n",
      " 44% 42/96 [00:27<00:32,  1.67it/s]\u001b[A\n",
      " 45% 43/96 [00:27<00:30,  1.71it/s]\u001b[A\n",
      " 46% 44/96 [00:28<00:29,  1.74it/s]\u001b[A\n",
      " 47% 45/96 [00:28<00:28,  1.77it/s]\u001b[A\n",
      " 48% 46/96 [00:29<00:27,  1.79it/s]\u001b[A\n",
      " 49% 47/96 [00:30<00:27,  1.80it/s]\u001b[A\n",
      " 50% 48/96 [00:30<00:26,  1.80it/s]\u001b[A\n",
      " 51% 49/96 [00:31<00:25,  1.81it/s]\u001b[A\n",
      " 52% 50/96 [00:31<00:25,  1.82it/s]\u001b[A\n",
      " 53% 51/96 [00:32<00:24,  1.83it/s]\u001b[A\n",
      " 54% 52/96 [00:32<00:24,  1.83it/s]\u001b[A\n",
      " 55% 53/96 [00:33<00:23,  1.82it/s]\u001b[A\n",
      " 56% 54/96 [00:33<00:23,  1.82it/s]\u001b[A\n",
      " 57% 55/96 [00:34<00:22,  1.82it/s]\u001b[A\n",
      " 58% 56/96 [00:35<00:21,  1.82it/s]\u001b[A\n",
      " 59% 57/96 [00:35<00:21,  1.83it/s]\u001b[A\n",
      " 60% 58/96 [00:36<00:20,  1.82it/s]\u001b[A\n",
      " 61% 59/96 [00:36<00:20,  1.82it/s]\u001b[A\n",
      " 62% 60/96 [00:37<00:19,  1.82it/s]\u001b[A\n",
      " 64% 61/96 [00:37<00:19,  1.82it/s]\u001b[A\n",
      " 65% 62/96 [00:38<00:18,  1.82it/s]\u001b[A\n",
      " 66% 63/96 [00:38<00:18,  1.82it/s]\u001b[A\n",
      " 67% 64/96 [00:39<00:14,  2.17it/s]\u001b[ASaving model checkpoint to checkpoints/iter_1/checkpoint-64\n",
      "Configuration saved in checkpoints/iter_1/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints/iter_1/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_1/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_1/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_1/checkpoint-32] due to args.save_total_limit\n",
      "\n",
      " 68% 65/96 [00:45<01:13,  2.38s/it]\u001b[A\n",
      " 69% 66/96 [00:46<00:54,  1.83s/it]\u001b[A\n",
      " 70% 67/96 [00:47<00:41,  1.44s/it]\u001b[A\n",
      " 71% 68/96 [00:47<00:32,  1.18s/it]\u001b[A\n",
      " 72% 69/96 [00:48<00:26,  1.01it/s]\u001b[A\n",
      " 73% 70/96 [00:48<00:22,  1.17it/s]\u001b[A\n",
      " 74% 71/96 [00:49<00:19,  1.31it/s]\u001b[A\n",
      " 75% 72/96 [00:49<00:16,  1.43it/s]\u001b[A\n",
      " 76% 73/96 [00:50<00:15,  1.53it/s]\u001b[A\n",
      " 77% 74/96 [00:50<00:13,  1.60it/s]\u001b[A\n",
      " 78% 75/96 [00:51<00:12,  1.67it/s]\u001b[A\n",
      " 79% 76/96 [00:51<00:11,  1.71it/s]\u001b[A\n",
      " 80% 77/96 [00:52<00:10,  1.75it/s]\u001b[A\n",
      " 81% 78/96 [00:53<00:10,  1.77it/s]\u001b[A\n",
      " 82% 79/96 [00:53<00:09,  1.79it/s]\u001b[A\n",
      " 83% 80/96 [00:54<00:08,  1.80it/s]\u001b[A\n",
      " 84% 81/96 [00:54<00:08,  1.80it/s]\u001b[A\n",
      " 85% 82/96 [00:55<00:07,  1.81it/s]\u001b[A\n",
      " 86% 83/96 [00:55<00:07,  1.82it/s]\u001b[A\n",
      " 88% 84/96 [00:56<00:06,  1.82it/s]\u001b[A\n",
      " 89% 85/96 [00:56<00:06,  1.81it/s]\u001b[A\n",
      " 90% 86/96 [00:57<00:05,  1.81it/s]\u001b[A\n",
      " 91% 87/96 [00:58<00:04,  1.83it/s]\u001b[A\n",
      " 92% 88/96 [00:58<00:04,  1.82it/s]\u001b[A\n",
      " 93% 89/96 [00:59<00:03,  1.82it/s]\u001b[A\n",
      " 94% 90/96 [00:59<00:03,  1.77it/s]\u001b[A\n",
      " 95% 91/96 [01:00<00:02,  1.79it/s]\u001b[A\n",
      " 96% 92/96 [01:00<00:02,  1.80it/s]\u001b[A\n",
      " 97% 93/96 [01:01<00:01,  1.81it/s]\u001b[A\n",
      " 98% 94/96 [01:01<00:01,  1.81it/s]\u001b[A\n",
      " 99% 95/96 [01:02<00:00,  1.81it/s]\u001b[A\n",
      "100% 96/96 [01:02<00:00,  2.17it/s]\u001b[ASaving model checkpoint to checkpoints/iter_1/checkpoint-96\n",
      "Configuration saved in checkpoints/iter_1/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints/iter_1/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_1/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_1/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_1/checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'train_runtime': 74.656, 'train_samples_per_second': 40.184, 'train_steps_per_second': 1.286, 'train_loss': 0.5369845231374105, 'epoch': 3.0}\n",
      "100% 76/76 [01:28<00:00, 14.07it/s]\n",
      "100% 96/96 [01:14<00:00,  1.29it/s]\n",
      "Training took time  0:01:14.673841\n",
      "***** Running Prediction *****\n",
      "  Num examples = 602\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/76 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 3/76 [00:00<00:04, 16.08it/s]\u001b[A\n",
      "  7% 5/76 [00:00<00:05, 13.74it/s]\u001b[A\n",
      "  9% 7/76 [00:00<00:05, 13.32it/s]\u001b[A\n",
      " 12% 9/76 [00:00<00:04, 13.42it/s]\u001b[A\n",
      " 14% 11/76 [00:00<00:04, 13.60it/s]\u001b[A\n",
      " 17% 13/76 [00:00<00:04, 13.62it/s]\u001b[A\n",
      " 20% 15/76 [00:01<00:04, 13.67it/s]\u001b[A\n",
      " 22% 17/76 [00:01<00:04, 13.74it/s]\u001b[A\n",
      " 25% 19/76 [00:01<00:04, 13.80it/s]\u001b[A\n",
      " 28% 21/76 [00:01<00:03, 13.84it/s]\u001b[A\n",
      " 30% 23/76 [00:01<00:03, 13.87it/s]\u001b[A\n",
      " 33% 25/76 [00:01<00:03, 13.94it/s]\u001b[A\n",
      " 36% 27/76 [00:01<00:03, 13.98it/s]\u001b[A\n",
      " 38% 29/76 [00:02<00:03, 13.94it/s]\u001b[A\n",
      " 41% 31/76 [00:02<00:03, 13.93it/s]\u001b[A\n",
      " 43% 33/76 [00:02<00:03, 13.96it/s]\u001b[A\n",
      " 46% 35/76 [00:02<00:02, 13.99it/s]\u001b[A\n",
      " 49% 37/76 [00:02<00:02, 14.01it/s]\u001b[A\n",
      " 51% 39/76 [00:02<00:02, 13.97it/s]\u001b[A\n",
      " 54% 41/76 [00:02<00:02, 13.96it/s]\u001b[A\n",
      " 57% 43/76 [00:03<00:02, 13.94it/s]\u001b[A\n",
      " 59% 45/76 [00:03<00:02, 13.96it/s]\u001b[A\n",
      " 62% 47/76 [00:03<00:02, 14.00it/s]\u001b[A\n",
      " 64% 49/76 [00:03<00:01, 13.99it/s]\u001b[A\n",
      " 67% 51/76 [00:03<00:01, 14.00it/s]\u001b[A\n",
      " 70% 53/76 [00:03<00:01, 13.97it/s]\u001b[A\n",
      " 72% 55/76 [00:03<00:01, 13.93it/s]\u001b[A\n",
      " 75% 57/76 [00:04<00:01, 13.91it/s]\u001b[A\n",
      " 78% 59/76 [00:04<00:01, 13.95it/s]\u001b[A\n",
      " 80% 61/76 [00:04<00:01, 13.98it/s]\u001b[A\n",
      " 83% 63/76 [00:04<00:00, 13.99it/s]\u001b[A\n",
      " 86% 65/76 [00:04<00:00, 13.94it/s]\u001b[A\n",
      " 88% 67/76 [00:04<00:00, 14.03it/s]\u001b[A\n",
      " 91% 69/76 [00:04<00:00, 14.03it/s]\u001b[A\n",
      " 93% 71/76 [00:05<00:00, 14.00it/s]\u001b[A\n",
      " 96% 73/76 [00:05<00:00, 13.97it/s]\u001b[A\n",
      " 99% 75/76 [00:05<00:00, 13.92it/s]\u001b[Aiteration 2\n",
      "dataset: bho\n",
      "dataset: callme\n",
      "dataset: scales complete\n",
      "dataset: bh\n",
      "dataset: scales train\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100% 76/76 [00:11<00:00,  6.86it/s]\n",
      "training bert_finetuned on adversarial omnibus\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Beginning Training!\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      "\n",
      "  0% 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "  1% 1/96 [00:00<00:49,  1.90it/s]\u001b[A\n",
      "  2% 2/96 [00:01<00:50,  1.85it/s]\u001b[A\n",
      "  3% 3/96 [00:01<00:50,  1.84it/s]\u001b[A\n",
      "  4% 4/96 [00:02<00:50,  1.84it/s]\u001b[A\n",
      "  5% 5/96 [00:02<00:49,  1.84it/s]\u001b[A\n",
      "  6% 6/96 [00:03<00:49,  1.83it/s]\u001b[A\n",
      "  7% 7/96 [00:03<00:48,  1.83it/s]\u001b[A\n",
      "  8% 8/96 [00:04<00:48,  1.83it/s]\u001b[A\n",
      "  9% 9/96 [00:04<00:47,  1.83it/s]\u001b[A\n",
      " 10% 10/96 [00:05<00:46,  1.83it/s]\u001b[A\n",
      " 11% 11/96 [00:05<00:46,  1.84it/s]\u001b[A\n",
      " 12% 12/96 [00:06<00:45,  1.84it/s]\u001b[A\n",
      " 14% 13/96 [00:07<00:45,  1.83it/s]\u001b[A\n",
      " 15% 14/96 [00:07<00:44,  1.82it/s]\u001b[A\n",
      " 16% 15/96 [00:08<00:44,  1.83it/s]\u001b[A\n",
      " 17% 16/96 [00:08<00:43,  1.83it/s]\u001b[A\n",
      " 18% 17/96 [00:09<00:43,  1.82it/s]\u001b[A\n",
      " 19% 18/96 [00:09<00:42,  1.82it/s]\u001b[A\n",
      " 20% 19/96 [00:10<00:42,  1.81it/s]\u001b[A\n",
      " 21% 20/96 [00:10<00:41,  1.81it/s]\u001b[A\n",
      " 22% 21/96 [00:11<00:41,  1.82it/s]\u001b[A\n",
      " 23% 22/96 [00:12<00:40,  1.82it/s]\u001b[A\n",
      " 24% 23/96 [00:12<00:40,  1.82it/s]\u001b[A\n",
      " 25% 24/96 [00:13<00:39,  1.82it/s]\u001b[A\n",
      " 26% 25/96 [00:13<00:39,  1.82it/s]\u001b[A\n",
      " 27% 26/96 [00:14<00:38,  1.82it/s]\u001b[A\n",
      " 28% 27/96 [00:14<00:37,  1.83it/s]\u001b[A\n",
      " 29% 28/96 [00:15<00:37,  1.82it/s]\u001b[A\n",
      " 30% 29/96 [00:15<00:36,  1.82it/s]\u001b[A\n",
      " 31% 30/96 [00:16<00:36,  1.82it/s]\u001b[A\n",
      " 32% 31/96 [00:16<00:35,  1.83it/s]\u001b[A\n",
      " 33% 32/96 [00:17<00:29,  2.19it/s]\u001b[ASaving model checkpoint to checkpoints/iter_2/checkpoint-32\n",
      "Configuration saved in checkpoints/iter_2/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints/iter_2/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_2/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_2/checkpoint-32/special_tokens_map.json\n",
      "\n",
      " 34% 33/96 [00:22<02:04,  1.98s/it]\u001b[A\n",
      " 35% 34/96 [00:23<01:36,  1.55s/it]\u001b[A\n",
      " 36% 35/96 [00:23<01:16,  1.25s/it]\u001b[A\n",
      " 38% 36/96 [00:24<01:02,  1.04s/it]\u001b[A\n",
      " 39% 37/96 [00:24<00:52,  1.12it/s]\u001b[A\n",
      " 40% 38/96 [00:25<00:45,  1.27it/s]\u001b[A\n",
      " 41% 39/96 [00:26<00:40,  1.40it/s]\u001b[A\n",
      " 42% 40/96 [00:26<00:37,  1.50it/s]\u001b[A\n",
      " 43% 41/96 [00:27<00:34,  1.58it/s]\u001b[A\n",
      " 44% 42/96 [00:27<00:32,  1.65it/s]\u001b[A\n",
      " 45% 43/96 [00:28<00:31,  1.70it/s]\u001b[A\n",
      " 46% 44/96 [00:28<00:29,  1.74it/s]\u001b[A\n",
      " 47% 45/96 [00:29<00:28,  1.76it/s]\u001b[A\n",
      " 48% 46/96 [00:29<00:28,  1.78it/s]\u001b[A\n",
      " 49% 47/96 [00:30<00:27,  1.79it/s]\u001b[A\n",
      " 50% 48/96 [00:31<00:27,  1.74it/s]\u001b[A\n",
      " 51% 49/96 [00:31<00:26,  1.76it/s]\u001b[A\n",
      " 52% 50/96 [00:32<00:25,  1.78it/s]\u001b[A\n",
      " 53% 51/96 [00:32<00:25,  1.80it/s]\u001b[A\n",
      " 54% 52/96 [00:33<00:24,  1.81it/s]\u001b[A\n",
      " 55% 53/96 [00:33<00:23,  1.82it/s]\u001b[A\n",
      " 56% 54/96 [00:34<00:23,  1.82it/s]\u001b[A\n",
      " 57% 55/96 [00:34<00:22,  1.83it/s]\u001b[A\n",
      " 58% 56/96 [00:35<00:21,  1.82it/s]\u001b[A\n",
      " 59% 57/96 [00:35<00:21,  1.82it/s]\u001b[A\n",
      " 60% 58/96 [00:36<00:20,  1.82it/s]\u001b[A\n",
      " 61% 59/96 [00:37<00:20,  1.83it/s]\u001b[A\n",
      " 62% 60/96 [00:37<00:19,  1.83it/s]\u001b[A\n",
      " 64% 61/96 [00:38<00:19,  1.82it/s]\u001b[A\n",
      " 65% 62/96 [00:38<00:18,  1.82it/s]\u001b[A\n",
      " 66% 63/96 [00:39<00:18,  1.83it/s]\u001b[A\n",
      " 67% 64/96 [00:39<00:14,  2.18it/s]\u001b[ASaving model checkpoint to checkpoints/iter_2/checkpoint-64\n",
      "Configuration saved in checkpoints/iter_2/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints/iter_2/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_2/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_2/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_2/checkpoint-32] due to args.save_total_limit\n",
      "\n",
      " 68% 65/96 [00:46<01:13,  2.38s/it]\u001b[A\n",
      " 69% 66/96 [00:46<00:55,  1.84s/it]\u001b[A\n",
      " 70% 67/96 [00:47<00:42,  1.45s/it]\u001b[A\n",
      " 71% 68/96 [00:48<00:32,  1.18s/it]\u001b[A\n",
      " 72% 69/96 [00:48<00:26,  1.01it/s]\u001b[A\n",
      " 73% 70/96 [00:49<00:22,  1.17it/s]\u001b[A\n",
      " 74% 71/96 [00:49<00:19,  1.31it/s]\u001b[A\n",
      " 75% 72/96 [00:50<00:16,  1.43it/s]\u001b[A\n",
      " 76% 73/96 [00:50<00:15,  1.53it/s]\u001b[A\n",
      " 77% 74/96 [00:51<00:13,  1.61it/s]\u001b[A\n",
      " 78% 75/96 [00:51<00:12,  1.67it/s]\u001b[A\n",
      " 79% 76/96 [00:52<00:11,  1.71it/s]\u001b[A\n",
      " 80% 77/96 [00:52<00:10,  1.74it/s]\u001b[A\n",
      " 81% 78/96 [00:53<00:10,  1.76it/s]\u001b[A\n",
      " 82% 79/96 [00:54<00:09,  1.79it/s]\u001b[A\n",
      " 83% 80/96 [00:54<00:08,  1.80it/s]\u001b[A\n",
      " 84% 81/96 [00:55<00:08,  1.80it/s]\u001b[A\n",
      " 85% 82/96 [00:55<00:07,  1.81it/s]\u001b[A\n",
      " 86% 83/96 [00:56<00:07,  1.81it/s]\u001b[A\n",
      " 88% 84/96 [00:56<00:06,  1.82it/s]\u001b[A\n",
      " 89% 85/96 [00:57<00:06,  1.82it/s]\u001b[A\n",
      " 90% 86/96 [00:57<00:05,  1.82it/s]\u001b[A\n",
      " 91% 87/96 [00:58<00:04,  1.82it/s]\u001b[A\n",
      " 92% 88/96 [00:58<00:04,  1.82it/s]\u001b[A\n",
      " 93% 89/96 [00:59<00:03,  1.82it/s]\u001b[A\n",
      " 94% 90/96 [01:00<00:03,  1.82it/s]\u001b[A\n",
      " 95% 91/96 [01:00<00:02,  1.82it/s]\u001b[A\n",
      " 96% 92/96 [01:01<00:02,  1.82it/s]\u001b[A\n",
      " 97% 93/96 [01:01<00:01,  1.81it/s]\u001b[A\n",
      " 98% 94/96 [01:02<00:01,  1.82it/s]\u001b[A\n",
      " 99% 95/96 [01:02<00:00,  1.83it/s]\u001b[A\n",
      "100% 96/96 [01:03<00:00,  2.18it/s]\u001b[ASaving model checkpoint to checkpoints/iter_2/checkpoint-96\n",
      "Configuration saved in checkpoints/iter_2/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints/iter_2/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_2/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_2/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_2/checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'train_runtime': 78.2772, 'train_samples_per_second': 38.325, 'train_steps_per_second': 1.226, 'train_loss': 0.5895928541819254, 'epoch': 3.0}\n",
      "100% 76/76 [02:58<00:00, 14.07it/s]\n",
      "100% 96/96 [01:18<00:00,  1.23it/s]\n",
      "Training took time  0:01:18.285610\n",
      "***** Running Prediction *****\n",
      "  Num examples = 602\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/76 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 3/76 [00:00<00:03, 22.75it/s]\u001b[A\n",
      "  8% 6/76 [00:00<00:04, 17.25it/s]\u001b[A\n",
      " 11% 8/76 [00:00<00:04, 15.91it/s]\u001b[A\n",
      " 13% 10/76 [00:00<00:04, 15.17it/s]\u001b[A\n",
      " 16% 12/76 [00:00<00:04, 14.70it/s]\u001b[A\n",
      " 18% 14/76 [00:00<00:04, 14.47it/s]\u001b[A\n",
      " 21% 16/76 [00:01<00:04, 14.37it/s]\u001b[A\n",
      " 24% 18/76 [00:01<00:04, 14.27it/s]\u001b[A\n",
      " 26% 20/76 [00:01<00:03, 14.19it/s]\u001b[A\n",
      " 29% 22/76 [00:01<00:03, 14.09it/s]\u001b[A\n",
      " 32% 24/76 [00:01<00:03, 14.01it/s]\u001b[A\n",
      " 34% 26/76 [00:01<00:03, 13.98it/s]\u001b[A\n",
      " 37% 28/76 [00:01<00:03, 14.03it/s]\u001b[A\n",
      " 39% 30/76 [00:02<00:03, 14.05it/s]\u001b[A\n",
      " 42% 32/76 [00:02<00:03, 14.00it/s]\u001b[A\n",
      " 45% 34/76 [00:02<00:03, 13.95it/s]\u001b[A\n",
      " 47% 36/76 [00:02<00:02, 13.98it/s]\u001b[A\n",
      " 50% 38/76 [00:02<00:02, 14.00it/s]\u001b[A\n",
      " 53% 40/76 [00:02<00:02, 14.02it/s]\u001b[A\n",
      " 55% 42/76 [00:02<00:02, 14.03it/s]\u001b[A\n",
      " 58% 44/76 [00:03<00:02, 14.01it/s]\u001b[A\n",
      " 61% 46/76 [00:03<00:02, 13.97it/s]\u001b[A\n",
      " 63% 48/76 [00:03<00:02, 14.00it/s]\u001b[A\n",
      " 66% 50/76 [00:03<00:01, 14.04it/s]\u001b[A\n",
      " 68% 52/76 [00:03<00:01, 14.00it/s]\u001b[A\n",
      " 71% 54/76 [00:03<00:01, 13.98it/s]\u001b[A\n",
      " 74% 56/76 [00:03<00:01, 13.96it/s]\u001b[A\n",
      " 76% 58/76 [00:04<00:01, 13.94it/s]\u001b[A\n",
      " 79% 60/76 [00:04<00:01, 13.93it/s]\u001b[A\n",
      " 82% 62/76 [00:04<00:01, 13.97it/s]\u001b[A\n",
      " 84% 64/76 [00:04<00:00, 14.02it/s]\u001b[A\n",
      " 87% 66/76 [00:04<00:00, 14.02it/s]\u001b[A\n",
      " 89% 68/76 [00:04<00:00, 14.02it/s]\u001b[A\n",
      " 92% 70/76 [00:04<00:00, 13.99it/s]\u001b[A\n",
      " 95% 72/76 [00:05<00:00, 13.93it/s]\u001b[A\n",
      " 97% 74/76 [00:05<00:00, 13.94it/s]\u001b[A\n",
      "100% 76/76 [00:05<00:00, 15.33it/s]\u001b[Aiteration 3\n",
      "dataset: bho\n",
      "dataset: callme\n",
      "dataset: scales complete\n",
      "dataset: bh\n",
      "dataset: scales train\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100% 76/76 [00:10<00:00,  7.07it/s]\n",
      "training bert_finetuned on adversarial omnibus\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Beginning Training!\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      "\n",
      "  0% 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "  1% 1/96 [00:00<00:50,  1.88it/s]\u001b[A\n",
      "  2% 2/96 [00:01<00:50,  1.85it/s]\u001b[A\n",
      "  3% 3/96 [00:01<00:50,  1.85it/s]\u001b[A\n",
      "  4% 4/96 [00:02<00:49,  1.85it/s]\u001b[A\n",
      "  5% 5/96 [00:02<00:49,  1.84it/s]\u001b[A\n",
      "  6% 6/96 [00:03<00:49,  1.83it/s]\u001b[A\n",
      "  7% 7/96 [00:03<00:48,  1.83it/s]\u001b[A\n",
      "  8% 8/96 [00:04<00:48,  1.82it/s]\u001b[A\n",
      "  9% 9/96 [00:04<00:47,  1.82it/s]\u001b[A\n",
      " 10% 10/96 [00:05<00:47,  1.82it/s]\u001b[A\n",
      " 11% 11/96 [00:06<00:46,  1.82it/s]\u001b[A\n",
      " 12% 12/96 [00:06<00:46,  1.82it/s]\u001b[A\n",
      " 14% 13/96 [00:07<00:45,  1.82it/s]\u001b[A\n",
      " 15% 14/96 [00:07<00:45,  1.82it/s]\u001b[A\n",
      " 16% 15/96 [00:08<00:44,  1.82it/s]\u001b[A\n",
      " 17% 16/96 [00:08<00:43,  1.82it/s]\u001b[A\n",
      " 18% 17/96 [00:09<00:43,  1.82it/s]\u001b[A\n",
      " 19% 18/96 [00:09<00:43,  1.81it/s]\u001b[A\n",
      " 20% 19/96 [00:10<00:42,  1.82it/s]\u001b[A\n",
      " 21% 20/96 [00:10<00:41,  1.81it/s]\u001b[A\n",
      " 22% 21/96 [00:11<00:41,  1.82it/s]\u001b[A\n",
      " 23% 22/96 [00:12<00:41,  1.80it/s]\u001b[A\n",
      " 24% 23/96 [00:12<00:40,  1.81it/s]\u001b[A\n",
      " 25% 24/96 [00:13<00:39,  1.81it/s]\u001b[A\n",
      " 26% 25/96 [00:13<00:39,  1.81it/s]\u001b[A\n",
      " 27% 26/96 [00:14<00:38,  1.81it/s]\u001b[A\n",
      " 28% 27/96 [00:14<00:38,  1.82it/s]\u001b[A\n",
      " 29% 28/96 [00:15<00:37,  1.82it/s]\u001b[A\n",
      " 30% 29/96 [00:15<00:36,  1.82it/s]\u001b[A\n",
      " 31% 30/96 [00:16<00:36,  1.82it/s]\u001b[A\n",
      " 32% 31/96 [00:17<00:35,  1.83it/s]\u001b[A\n",
      " 33% 32/96 [00:17<00:29,  2.18it/s]\u001b[ASaving model checkpoint to checkpoints/iter_3/checkpoint-32\n",
      "Configuration saved in checkpoints/iter_3/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints/iter_3/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_3/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_3/checkpoint-32/special_tokens_map.json\n",
      "\n",
      " 34% 33/96 [00:22<02:04,  1.98s/it]\u001b[A\n",
      " 35% 34/96 [00:23<01:36,  1.55s/it]\u001b[A\n",
      " 36% 35/96 [00:23<01:16,  1.25s/it]\u001b[A\n",
      " 38% 36/96 [00:24<01:02,  1.04s/it]\u001b[A\n",
      " 39% 37/96 [00:24<00:52,  1.12it/s]\u001b[A\n",
      " 40% 38/96 [00:25<00:45,  1.27it/s]\u001b[A\n",
      " 41% 39/96 [00:26<00:40,  1.40it/s]\u001b[A\n",
      " 42% 40/96 [00:26<00:37,  1.50it/s]\u001b[A\n",
      " 43% 41/96 [00:27<00:34,  1.58it/s]\u001b[A\n",
      " 44% 42/96 [00:27<00:32,  1.65it/s]\u001b[A\n",
      " 45% 43/96 [00:28<00:31,  1.70it/s]\u001b[A\n",
      " 46% 44/96 [00:28<00:29,  1.73it/s]\u001b[A\n",
      " 47% 45/96 [00:29<00:29,  1.76it/s]\u001b[A\n",
      " 48% 46/96 [00:29<00:28,  1.77it/s]\u001b[A\n",
      " 49% 47/96 [00:30<00:27,  1.78it/s]\u001b[A\n",
      " 50% 48/96 [00:31<00:26,  1.80it/s]\u001b[A\n",
      " 51% 49/96 [00:31<00:26,  1.80it/s]\u001b[A\n",
      " 52% 50/96 [00:32<00:25,  1.80it/s]\u001b[A\n",
      " 53% 51/96 [00:32<00:24,  1.81it/s]\u001b[A\n",
      " 54% 52/96 [00:33<00:24,  1.81it/s]\u001b[A\n",
      " 55% 53/96 [00:33<00:23,  1.82it/s]\u001b[A\n",
      " 56% 54/96 [00:34<00:23,  1.82it/s]\u001b[A\n",
      " 57% 55/96 [00:34<00:22,  1.82it/s]\u001b[A\n",
      " 58% 56/96 [00:35<00:22,  1.82it/s]\u001b[A\n",
      " 59% 57/96 [00:35<00:21,  1.82it/s]\u001b[A\n",
      " 60% 58/96 [00:36<00:20,  1.82it/s]\u001b[A\n",
      " 61% 59/96 [00:37<00:20,  1.82it/s]\u001b[A\n",
      " 62% 60/96 [00:37<00:19,  1.81it/s]\u001b[A\n",
      " 64% 61/96 [00:38<00:19,  1.82it/s]\u001b[A\n",
      " 65% 62/96 [00:38<00:18,  1.82it/s]\u001b[A\n",
      " 66% 63/96 [00:39<00:18,  1.83it/s]\u001b[A\n",
      " 67% 64/96 [00:39<00:14,  2.18it/s]\u001b[ASaving model checkpoint to checkpoints/iter_3/checkpoint-64\n",
      "Configuration saved in checkpoints/iter_3/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints/iter_3/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_3/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_3/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_3/checkpoint-32] due to args.save_total_limit\n",
      "\n",
      " 68% 65/96 [00:47<01:21,  2.62s/it]\u001b[A\n",
      " 69% 66/96 [00:47<00:59,  2.00s/it]\u001b[A\n",
      " 70% 67/96 [00:48<00:45,  1.56s/it]\u001b[A\n",
      " 71% 68/96 [00:48<00:35,  1.26s/it]\u001b[A\n",
      " 72% 69/96 [00:49<00:28,  1.04s/it]\u001b[A\n",
      " 73% 70/96 [00:49<00:23,  1.12it/s]\u001b[A\n",
      " 74% 71/96 [00:50<00:19,  1.26it/s]\u001b[A\n",
      " 75% 72/96 [00:50<00:17,  1.39it/s]\u001b[A\n",
      " 76% 73/96 [00:51<00:15,  1.50it/s]\u001b[A\n",
      " 77% 74/96 [00:52<00:13,  1.59it/s]\u001b[A\n",
      " 78% 75/96 [00:52<00:12,  1.65it/s]\u001b[A\n",
      " 79% 76/96 [00:53<00:11,  1.70it/s]\u001b[A\n",
      " 80% 77/96 [00:53<00:10,  1.73it/s]\u001b[A\n",
      " 81% 78/96 [00:54<00:10,  1.76it/s]\u001b[A\n",
      " 82% 79/96 [00:54<00:09,  1.78it/s]\u001b[A\n",
      " 83% 80/96 [00:55<00:08,  1.79it/s]\u001b[A\n",
      " 84% 81/96 [00:55<00:08,  1.80it/s]\u001b[A\n",
      " 85% 82/96 [00:56<00:07,  1.81it/s]\u001b[A\n",
      " 86% 83/96 [00:57<00:07,  1.82it/s]\u001b[A\n",
      " 88% 84/96 [00:57<00:06,  1.82it/s]\u001b[A\n",
      " 89% 85/96 [00:58<00:06,  1.81it/s]\u001b[A\n",
      " 90% 86/96 [00:58<00:05,  1.81it/s]\u001b[A\n",
      " 91% 87/96 [00:59<00:04,  1.82it/s]\u001b[A\n",
      " 92% 88/96 [00:59<00:04,  1.82it/s]\u001b[A\n",
      " 93% 89/96 [01:00<00:03,  1.81it/s]\u001b[A\n",
      " 94% 90/96 [01:00<00:03,  1.81it/s]\u001b[A\n",
      " 95% 91/96 [01:01<00:02,  1.81it/s]\u001b[A\n",
      " 96% 92/96 [01:01<00:02,  1.81it/s]\u001b[A\n",
      " 97% 93/96 [01:02<00:01,  1.82it/s]\u001b[A\n",
      " 98% 94/96 [01:03<00:01,  1.82it/s]\u001b[A\n",
      " 99% 95/96 [01:03<00:00,  1.82it/s]\u001b[A\n",
      "100% 96/96 [01:03<00:00,  2.17it/s]\u001b[ASaving model checkpoint to checkpoints/iter_3/checkpoint-96\n",
      "Configuration saved in checkpoints/iter_3/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints/iter_3/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_3/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_3/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_3/checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'train_runtime': 75.2735, 'train_samples_per_second': 39.855, 'train_steps_per_second': 1.275, 'train_loss': 0.5692944129308065, 'epoch': 3.0}\n",
      "100% 76/76 [04:27<00:00, 14.07it/s]\n",
      "100% 96/96 [01:16<00:00,  1.25it/s]\n",
      "Training took time  0:01:16.665722\n",
      "***** Running Prediction *****\n",
      "  Num examples = 602\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/76 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 3/76 [00:00<00:04, 16.87it/s]\u001b[A\n",
      "  7% 5/76 [00:00<00:04, 14.35it/s]\u001b[A\n",
      "  9% 7/76 [00:00<00:04, 13.90it/s]\u001b[A\n",
      " 12% 9/76 [00:00<00:04, 13.84it/s]\u001b[A\n",
      " 14% 11/76 [00:00<00:04, 13.85it/s]\u001b[A\n",
      " 17% 13/76 [00:00<00:04, 13.85it/s]\u001b[A\n",
      " 20% 15/76 [00:01<00:04, 13.83it/s]\u001b[A\n",
      " 22% 17/76 [00:01<00:04, 13.83it/s]\u001b[A\n",
      " 25% 19/76 [00:01<00:04, 13.85it/s]\u001b[A\n",
      " 28% 21/76 [00:01<00:03, 13.86it/s]\u001b[A\n",
      " 30% 23/76 [00:01<00:03, 13.84it/s]\u001b[A\n",
      " 33% 25/76 [00:01<00:03, 13.90it/s]\u001b[A\n",
      " 36% 27/76 [00:01<00:03, 13.94it/s]\u001b[A\n",
      " 38% 29/76 [00:02<00:03, 13.94it/s]\u001b[A\n",
      " 41% 31/76 [00:02<00:03, 13.94it/s]\u001b[A\n",
      " 43% 33/76 [00:02<00:03, 14.01it/s]\u001b[A\n",
      " 46% 35/76 [00:02<00:02, 14.07it/s]\u001b[A\n",
      " 49% 37/76 [00:02<00:02, 14.10it/s]\u001b[A\n",
      " 51% 39/76 [00:02<00:02, 14.13it/s]\u001b[A\n",
      " 54% 41/76 [00:02<00:02, 14.13it/s]\u001b[A\n",
      " 57% 43/76 [00:03<00:02, 14.15it/s]\u001b[A\n",
      " 59% 45/76 [00:03<00:02, 14.15it/s]\u001b[A\n",
      " 62% 47/76 [00:03<00:02, 14.07it/s]\u001b[A\n",
      " 64% 49/76 [00:03<00:01, 14.04it/s]\u001b[A\n",
      " 67% 51/76 [00:03<00:01, 14.06it/s]\u001b[A\n",
      " 70% 53/76 [00:03<00:01, 14.12it/s]\u001b[A\n",
      " 72% 55/76 [00:03<00:01, 14.15it/s]\u001b[A\n",
      " 75% 57/76 [00:04<00:01, 14.12it/s]\u001b[A\n",
      " 78% 59/76 [00:04<00:01, 14.13it/s]\u001b[A\n",
      " 80% 61/76 [00:04<00:01, 14.13it/s]\u001b[A\n",
      " 83% 63/76 [00:04<00:00, 14.12it/s]\u001b[A\n",
      " 86% 65/76 [00:04<00:00, 14.02it/s]\u001b[A\n",
      " 88% 67/76 [00:04<00:00, 14.02it/s]\u001b[A\n",
      " 91% 69/76 [00:04<00:00, 14.07it/s]\u001b[A\n",
      " 93% 71/76 [00:05<00:00, 14.10it/s]\u001b[A\n",
      " 96% 73/76 [00:05<00:00, 14.08it/s]\u001b[A\n",
      " 99% 75/76 [00:05<00:00, 14.03it/s]\u001b[Aiteration 4\n",
      "dataset: bho\n",
      "dataset: callme\n",
      "dataset: scales complete\n",
      "dataset: bh\n",
      "dataset: scales train\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100% 76/76 [00:10<00:00,  6.91it/s]\n",
      "training bert_finetuned on adversarial omnibus\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Beginning Training!\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      "\n",
      "  0% 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "  1% 1/96 [00:00<00:51,  1.84it/s]\u001b[A\n",
      "  2% 2/96 [00:01<00:51,  1.84it/s]\u001b[A\n",
      "  3% 3/96 [00:01<00:50,  1.83it/s]\u001b[A\n",
      "  4% 4/96 [00:02<00:50,  1.83it/s]\u001b[A\n",
      "  5% 5/96 [00:02<00:49,  1.83it/s]\u001b[A\n",
      "  6% 6/96 [00:03<00:49,  1.84it/s]\u001b[A\n",
      "  7% 7/96 [00:03<00:48,  1.84it/s]\u001b[A\n",
      "  8% 8/96 [00:04<00:48,  1.83it/s]\u001b[A\n",
      "  9% 9/96 [00:04<00:47,  1.82it/s]\u001b[A\n",
      " 10% 10/96 [00:05<00:47,  1.82it/s]\u001b[A\n",
      " 11% 11/96 [00:06<00:46,  1.83it/s]\u001b[A\n",
      " 12% 12/96 [00:06<00:46,  1.82it/s]\u001b[A\n",
      " 14% 13/96 [00:07<00:45,  1.81it/s]\u001b[A\n",
      " 15% 14/96 [00:07<00:45,  1.81it/s]\u001b[A\n",
      " 16% 15/96 [00:08<00:44,  1.82it/s]\u001b[A\n",
      " 17% 16/96 [00:08<00:44,  1.82it/s]\u001b[A\n",
      " 18% 17/96 [00:09<00:43,  1.82it/s]\u001b[A\n",
      " 19% 18/96 [00:09<00:42,  1.82it/s]\u001b[A\n",
      " 20% 19/96 [00:10<00:42,  1.82it/s]\u001b[A\n",
      " 21% 20/96 [00:10<00:41,  1.82it/s]\u001b[A\n",
      " 22% 21/96 [00:11<00:41,  1.82it/s]\u001b[A\n",
      " 23% 22/96 [00:12<00:40,  1.82it/s]\u001b[A\n",
      " 24% 23/96 [00:12<00:40,  1.82it/s]\u001b[A\n",
      " 25% 24/96 [00:13<00:39,  1.81it/s]\u001b[A\n",
      " 26% 25/96 [00:13<00:39,  1.81it/s]\u001b[A\n",
      " 27% 26/96 [00:14<00:38,  1.82it/s]\u001b[A\n",
      " 28% 27/96 [00:14<00:37,  1.82it/s]\u001b[A\n",
      " 29% 28/96 [00:15<00:37,  1.81it/s]\u001b[A\n",
      " 30% 29/96 [00:15<00:36,  1.81it/s]\u001b[A\n",
      " 31% 30/96 [00:16<00:36,  1.82it/s]\u001b[A\n",
      " 32% 31/96 [00:17<00:35,  1.82it/s]\u001b[A\n",
      " 33% 32/96 [00:17<00:29,  2.18it/s]\u001b[ASaving model checkpoint to checkpoints/iter_4/checkpoint-32\n",
      "Configuration saved in checkpoints/iter_4/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints/iter_4/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_4/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_4/checkpoint-32/special_tokens_map.json\n",
      "\n",
      " 34% 33/96 [00:22<01:58,  1.89s/it]\u001b[A\n",
      " 35% 34/96 [00:23<01:32,  1.48s/it]\u001b[A\n",
      " 36% 35/96 [00:23<01:13,  1.20s/it]\u001b[A\n",
      " 38% 36/96 [00:24<01:00,  1.01s/it]\u001b[A\n",
      " 39% 37/96 [00:24<00:51,  1.15it/s]\u001b[A\n",
      " 40% 38/96 [00:25<00:45,  1.29it/s]\u001b[A\n",
      " 41% 39/96 [00:25<00:40,  1.41it/s]\u001b[A\n",
      " 42% 40/96 [00:26<00:36,  1.52it/s]\u001b[A\n",
      " 43% 41/96 [00:26<00:34,  1.60it/s]\u001b[A\n",
      " 44% 42/96 [00:27<00:32,  1.65it/s]\u001b[A\n",
      " 45% 43/96 [00:27<00:31,  1.70it/s]\u001b[A\n",
      " 46% 44/96 [00:28<00:29,  1.74it/s]\u001b[A\n",
      " 47% 45/96 [00:29<00:28,  1.76it/s]\u001b[A\n",
      " 48% 46/96 [00:29<00:28,  1.78it/s]\u001b[A\n",
      " 49% 47/96 [00:30<00:27,  1.79it/s]\u001b[A\n",
      " 50% 48/96 [00:30<00:26,  1.79it/s]\u001b[A\n",
      " 51% 49/96 [00:31<00:26,  1.80it/s]\u001b[A\n",
      " 52% 50/96 [00:31<00:25,  1.81it/s]\u001b[A\n",
      " 53% 51/96 [00:32<00:24,  1.81it/s]\u001b[A\n",
      " 54% 52/96 [00:32<00:24,  1.80it/s]\u001b[A\n",
      " 55% 53/96 [00:33<00:23,  1.81it/s]\u001b[A\n",
      " 56% 54/96 [00:34<00:23,  1.82it/s]\u001b[A\n",
      " 57% 55/96 [00:34<00:22,  1.82it/s]\u001b[A\n",
      " 58% 56/96 [00:35<00:22,  1.82it/s]\u001b[A\n",
      " 59% 57/96 [00:35<00:21,  1.82it/s]\u001b[A\n",
      " 60% 58/96 [00:36<00:20,  1.83it/s]\u001b[A\n",
      " 61% 59/96 [00:36<00:20,  1.83it/s]\u001b[A\n",
      " 62% 60/96 [00:37<00:19,  1.83it/s]\u001b[A\n",
      " 64% 61/96 [00:37<00:19,  1.82it/s]\u001b[A\n",
      " 65% 62/96 [00:38<00:18,  1.82it/s]\u001b[A\n",
      " 66% 63/96 [00:38<00:18,  1.82it/s]\u001b[A\n",
      " 67% 64/96 [00:39<00:14,  2.17it/s]\u001b[ASaving model checkpoint to checkpoints/iter_4/checkpoint-64\n",
      "Configuration saved in checkpoints/iter_4/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints/iter_4/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_4/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_4/checkpoint-64/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_4/checkpoint-32] due to args.save_total_limit\n",
      "\n",
      " 68% 65/96 [00:46<01:20,  2.58s/it]\u001b[A\n",
      " 69% 66/96 [00:47<00:59,  1.97s/it]\u001b[A\n",
      " 70% 67/96 [00:47<00:44,  1.55s/it]\u001b[A\n",
      " 71% 68/96 [00:48<00:34,  1.25s/it]\u001b[A\n",
      " 72% 69/96 [00:48<00:27,  1.04s/it]\u001b[A\n",
      " 73% 70/96 [00:49<00:23,  1.13it/s]\u001b[A\n",
      " 74% 71/96 [00:50<00:19,  1.27it/s]\u001b[A\n",
      " 75% 72/96 [00:50<00:17,  1.40it/s]\u001b[A\n",
      " 76% 73/96 [00:51<00:15,  1.51it/s]\u001b[A\n",
      " 77% 74/96 [00:51<00:13,  1.58it/s]\u001b[A\n",
      " 78% 75/96 [00:52<00:12,  1.65it/s]\u001b[A\n",
      " 79% 76/96 [00:52<00:11,  1.70it/s]\u001b[A\n",
      " 80% 77/96 [00:53<00:10,  1.74it/s]\u001b[A\n",
      " 81% 78/96 [00:53<00:10,  1.76it/s]\u001b[A\n",
      " 82% 79/96 [00:54<00:09,  1.78it/s]\u001b[A\n",
      " 83% 80/96 [00:54<00:08,  1.80it/s]\u001b[A\n",
      " 84% 81/96 [00:55<00:08,  1.81it/s]\u001b[A\n",
      " 85% 82/96 [00:56<00:07,  1.81it/s]\u001b[A\n",
      " 86% 83/96 [00:56<00:07,  1.82it/s]\u001b[A\n",
      " 88% 84/96 [00:57<00:06,  1.82it/s]\u001b[A\n",
      " 89% 85/96 [00:57<00:06,  1.82it/s]\u001b[A\n",
      " 90% 86/96 [00:58<00:05,  1.81it/s]\u001b[A\n",
      " 91% 87/96 [00:58<00:04,  1.82it/s]\u001b[A\n",
      " 92% 88/96 [00:59<00:04,  1.83it/s]\u001b[A\n",
      " 93% 89/96 [00:59<00:03,  1.82it/s]\u001b[A\n",
      " 94% 90/96 [01:00<00:03,  1.82it/s]\u001b[A\n",
      " 95% 91/96 [01:01<00:02,  1.82it/s]\u001b[A\n",
      " 96% 92/96 [01:01<00:02,  1.83it/s]\u001b[A\n",
      " 97% 93/96 [01:02<00:01,  1.83it/s]\u001b[A\n",
      " 98% 94/96 [01:02<00:01,  1.82it/s]\u001b[A\n",
      " 99% 95/96 [01:03<00:00,  1.82it/s]\u001b[A\n",
      "100% 96/96 [01:03<00:00,  2.17it/s]\u001b[ASaving model checkpoint to checkpoints/iter_4/checkpoint-96\n",
      "Configuration saved in checkpoints/iter_4/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints/iter_4/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/iter_4/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/iter_4/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/iter_4/checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'train_runtime': 75.4701, 'train_samples_per_second': 39.751, 'train_steps_per_second': 1.272, 'train_loss': 0.5179657141367594, 'epoch': 3.0}\n",
      "100% 76/76 [05:55<00:00, 14.07it/s]\n",
      "100% 96/96 [01:15<00:00,  1.26it/s]\n",
      "Training took time  0:01:15.934396\n",
      "***** Running Prediction *****\n",
      "  Num examples = 602\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/76 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 3/76 [00:00<00:03, 21.77it/s]\u001b[A\n",
      "  8% 6/76 [00:00<00:04, 17.13it/s]\u001b[A\n",
      " 11% 8/76 [00:00<00:04, 15.81it/s]\u001b[A\n",
      " 13% 10/76 [00:00<00:04, 15.13it/s]\u001b[A\n",
      " 16% 12/76 [00:00<00:04, 14.71it/s]\u001b[A\n",
      " 18% 14/76 [00:00<00:04, 14.55it/s]\u001b[A\n",
      " 21% 16/76 [00:01<00:04, 14.44it/s]\u001b[A\n",
      " 24% 18/76 [00:01<00:04, 14.34it/s]\u001b[A\n",
      " 26% 20/76 [00:01<00:03, 14.23it/s]\u001b[A\n",
      " 29% 22/76 [00:01<00:03, 14.14it/s]\u001b[A\n",
      " 32% 24/76 [00:01<00:03, 14.08it/s]\u001b[A\n",
      " 34% 26/76 [00:01<00:03, 14.04it/s]\u001b[A\n",
      " 37% 28/76 [00:01<00:03, 14.06it/s]\u001b[A\n",
      " 39% 30/76 [00:02<00:03, 14.04it/s]\u001b[A\n",
      " 42% 32/76 [00:02<00:03, 14.01it/s]\u001b[A\n",
      " 45% 34/76 [00:02<00:03, 14.00it/s]\u001b[A\n",
      " 47% 36/76 [00:02<00:02, 13.98it/s]\u001b[A\n",
      " 50% 38/76 [00:02<00:02, 13.95it/s]\u001b[A\n",
      " 53% 40/76 [00:02<00:02, 13.91it/s]\u001b[A\n",
      " 55% 42/76 [00:02<00:02, 13.95it/s]\u001b[A\n",
      " 58% 44/76 [00:03<00:02, 13.96it/s]\u001b[A\n",
      " 61% 46/76 [00:03<00:02, 13.94it/s]\u001b[A\n",
      " 63% 48/76 [00:03<00:02, 13.92it/s]\u001b[A\n",
      " 66% 50/76 [00:03<00:01, 13.90it/s]\u001b[A\n",
      " 68% 52/76 [00:03<00:01, 13.96it/s]\u001b[A\n",
      " 71% 54/76 [00:03<00:01, 14.01it/s]\u001b[A\n",
      " 74% 56/76 [00:03<00:01, 14.03it/s]\u001b[A\n",
      " 76% 58/76 [00:04<00:01, 13.95it/s]\u001b[A\n",
      " 79% 60/76 [00:04<00:01, 13.92it/s]\u001b[A\n",
      " 82% 62/76 [00:04<00:01, 13.94it/s]\u001b[A\n",
      " 84% 64/76 [00:04<00:00, 13.95it/s]\u001b[A\n",
      " 87% 66/76 [00:04<00:00, 13.95it/s]\u001b[A\n",
      " 89% 68/76 [00:04<00:00, 13.95it/s]\u001b[A\n",
      " 92% 70/76 [00:04<00:00, 13.99it/s]\u001b[A\n",
      " 95% 72/76 [00:05<00:00, 14.01it/s]\u001b[A\n",
      "100% 76/76 [06:01<00:00,  4.76s/it]\n",
      "100% 76/76 [00:05<00:00, 12.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# run the training -> 5 random splits of the dataset\n",
    "!python code/classify.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123374,
     "status": "ok",
     "timestamp": 1643456293661,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "U3tPa_cwb8CQ",
    "outputId": "56372a3a-ae8a-4aa0-dc19-0151d3cec6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "dataset: bho\n",
      "dataset: callme\n",
      "dataset: scales complete\n",
      "dataset: bh\n",
      "dataset: scales train\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "training bert_finetuned on adversarial omnibus\n",
      "Beginning Training!\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1602\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 153\n",
      " 33% 51/153 [00:28<00:46,  2.21it/s]Saving model checkpoint to checkpoints/final_model/checkpoint-51\n",
      "Configuration saved in checkpoints/final_model/checkpoint-51/config.json\n",
      "Model weights saved in checkpoints/final_model/checkpoint-51/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/final_model/checkpoint-51/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/final_model/checkpoint-51/special_tokens_map.json\n",
      " 67% 102/153 [01:02<00:22,  2.24it/s]Saving model checkpoint to checkpoints/final_model/checkpoint-102\n",
      "Configuration saved in checkpoints/final_model/checkpoint-102/config.json\n",
      "Model weights saved in checkpoints/final_model/checkpoint-102/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/final_model/checkpoint-102/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/final_model/checkpoint-102/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/final_model/checkpoint-51] due to args.save_total_limit\n",
      "100% 153/153 [01:36<00:00,  2.23it/s]Saving model checkpoint to checkpoints/final_model/checkpoint-153\n",
      "Configuration saved in checkpoints/final_model/checkpoint-153/config.json\n",
      "Model weights saved in checkpoints/final_model/checkpoint-153/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoints/final_model/checkpoint-153/tokenizer_config.json\n",
      "Special tokens file saved in checkpoints/final_model/checkpoint-153/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoints/final_model/checkpoint-102] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 101.2785, 'train_samples_per_second': 47.453, 'train_steps_per_second': 1.511, 'train_loss': 0.4841644686031965, 'epoch': 3.0}\n",
      "100% 153/153 [01:41<00:00,  1.51it/s]\n",
      "Training took time  0:01:41.289502\n"
     ]
    }
   ],
   "source": [
    "# train the model with the whole dataset\n",
    "!python code/train_final_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 413115,
     "status": "ok",
     "timestamp": 1643456999726,
     "user": {
      "displayName": "Lorenzo Betti",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04278478979303365527"
     },
     "user_tz": -60
    },
    "id": "5UX_e4qTl98L"
   },
   "outputs": [],
   "source": [
    "!cp -r checkpoints /content/drive/MyDrive/WASABI_gender_experiments/utils/reproduce_sexism_classifier/\n",
    "!cp results/all_runs.json /content/drive/MyDrive/WASABI_gender_experiments/utils/reproduce_sexism_classifier/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcHGZnUkl9_B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7nVt9gemOM8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOg89Z4b0ke+01k6tVY4xj2",
   "collapsed_sections": [],
   "name": "1.7.0_def_train_sexism_classifier.ipynb",
   "provenance": [
    {
     "file_id": "1ip_yfygDudF1KGk3KlDr-swpXIZ5poeX",
     "timestamp": 1643131171333
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
